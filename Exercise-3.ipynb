{"cells":[{"cell_type":"markdown","metadata":{},"source":["## TMA4215 - Exercise 3 ## \n","*Group: Hanna Heshmati Rød, Karine Austbø Grande and Thea Boge*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["#### Problem 1"]},{"cell_type":"markdown","metadata":{},"source":["#### a)\n","\n","1. $ f(\\mathbf{x}) = \\|\\mathbf{x}\\|_2 $\n","\n","This is the Euclidean norm (also known as the $L_2$ norm) of the vector $\\mathbf{x}$, defined as:\n","$$\n","f(\\mathbf{x}) = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} = \\left( \\mathbf{x}^T \\mathbf{x} \\right)^{1/2}\n","$$\n","\n","First we express $ f(\\mathbf{x}) $ in a differentiable form:\n","  $$\n","  f(\\mathbf{x}) = \\sqrt{\\mathbf{x}^T \\mathbf{x}}\n","  $$\n","\n","Now we differentiate with respect to $\\mathbf{x}$ using the chain rule. The derivative of $ \\sqrt{u} $ is $ \\frac{1}{2\\sqrt{u}} $:\n","  $$\n","  \\nabla f(\\mathbf{x}) = \\frac{1}{2 \\|\\mathbf{x}\\|_2} \\cdot 2\\mathbf{x} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n","  $$\n","\n","Thus, the gradient is:\n","$$\n","\\nabla f(\\mathbf{x}) = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n","$$\n","\n","\n","2. $ F(\\mathbf{x}) = A \\mathbf{x} $\n","\n","Here, $F(\\mathbf{x})$ is a matrix-vector multiplication, where $ A \\in \\mathbb{R}^{n \\times n} $.\n","\n","We need to compute the Jacobian matrix $ J_F(\\mathbf{x}) $. The Jacobian matrix for a vector-valued function is a matrix of partial derivatives.\n","  \n","Each component of $ F(\\mathbf{x}) = A \\mathbf{x} $ is a linear combination of the components of $\\mathbf{x}$, and the partial derivative of each output component with respect to each input component is the corresponding element of $A$.\n","  \n","In matrix form, the Jacobian is:\n","  $$\n","  J_F(\\mathbf{x}) = A\n","  $$\n","\n","Thus, the Jacobian matrix is simply the matrix $A$, because $A\\mathbf{x}$ is a linear transformation.\n","\n","\n","3. $ f(\\mathbf{x}) = \\|A \\mathbf{x}\\|_2^2 $\n","\n","This is the squared Euclidean norm of the vector $ A\\mathbf{x} $, defined as:\n","$$\n","f(\\mathbf{x}) = (A \\mathbf{x})^T (A \\mathbf{x}) = \\mathbf{x}^T A^T A \\mathbf{x}\n","$$\n","\n","To start of we rewrite the function $ f(\\mathbf{x}) $:\n","  $$\n","  f(\\mathbf{x}) = (A\\mathbf{x})^T (A\\mathbf{x}) = \\mathbf{x}^T A^T A \\mathbf{x}\n","  $$\n","  \n","The gradient of a quadratic form $ \\mathbf{x}^T B \\mathbf{x} $ is given by:\n","  $$\n","  \\nabla_{\\mathbf{x}} \\left( \\mathbf{x}^T B \\mathbf{x} \\right) = 2 B \\mathbf{x}\n","  $$\n","  \n","Here, $ B = A^T A $, so the gradient is:\n","  $$\n","  \\nabla f(\\mathbf{x}) = 2 A^T A \\mathbf{x}\n","  $$\n","\n","Thus, the gradient is:\n","$$\n","\\nabla f(\\mathbf{x}) = 2 A^T A \\mathbf{x}\n","$$\n","\n","\n","4. $ f(\\mathbf{x}) = (\\mathbf{A}\\mathbf{x}, \\mathbf{x}) = \\mathbf{x}^T A^T \\mathbf{x} $\n","\n","This is a quadratic form involving the matrix $A^T$ and the vector $\\mathbf{x}$.\n","\n","We express the function as:\n","  $$\n","  f(\\mathbf{x}) = \\mathbf{x}^T A^T \\mathbf{x}\n","  $$\n","  \n","The gradient of a quadratic form $ \\mathbf{x}^T B \\mathbf{x} $ is given by:\n","  $$\n","  \\nabla_{\\mathbf{x}} \\left( \\mathbf{x}^T B \\mathbf{x} \\right) = (B + B^T) \\mathbf{x}\n","  $$"]},{"cell_type":"markdown","metadata":{},"source":["#### b)\n","\n","If $ A $ is symmetric ($ A = A^T $), and we have from ealrier that $ B = A^T $, the gradient simplifies to:\n","$$\n","\\nabla f(\\mathbf{x}) = 2 A \\mathbf{x}\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["#### c)\n","\n","Assume $ A $ is symmetric, meaning $ A = A^T $.\n","\n","3. $ f(\\mathbf{x}) = \\|A \\mathbf{x}\\|_2^2 = \\mathbf{x}^T A^T A \\mathbf{x} $\n","\n","The gradient of the function, calculated earlier, is:\n","\n","$$\n","\\nabla f(\\mathbf{x}) = 2 A^T A \\mathbf{x} = 2 A^2 \\mathbf{x}\n","$$\n","\n","To compute the Hessian, we differentiate the gradient again with respect to $\\mathbf{x}$:\n","\n","$$\n","\\nabla^2 f(\\mathbf{x}) = 2 A^T A = 2 A^2\n","$$\n","\n","Thus, the Hessian for the third function is:\n","$$\n","\\nabla^2 f(\\mathbf{x}) = 2 A^2\n","$$\n","\n","\n","4. $ f(\\mathbf{x}) = (\\mathbf{A} \\mathbf{x}, \\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} $\n","\n","Again, the gradient of the given function, calculated previously, is:\n","\n","$$\n","\\nabla f(\\mathbf{x}) = 2 A \\mathbf{x}\n","$$\n","\n","Now we compute the Hessian by taking the gradient once again:\n","\n","$$\n","\\nabla^2 f(\\mathbf{x}) = 2 A\n","$$\n","\n","which is the Hessian for the fourth function."]},{"cell_type":"markdown","metadata":{},"source":["#### d)"]},{"cell_type":"markdown","metadata":{},"source":["#### Problem 2\n","\n","We consider the linear system of equations $A\\bf{x} = \\bf{b}$, where\n","$$A = \\left( \\begin{matrix}\n","3 & -1 & -c \\\\\n","-1 & 3 & 0 \\\\\n","-c & 0 & 3 \\\\\n","\\end{matrix} \\right)$$\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### a)\n","We want to reformulate the system of equations as a fix point equation \n","$$ A\\bf{x} = \\bf{b} \\iff  \\bf{x} = B\\bf{x} + \\bf{f}$$\n","with the iteration matrix B and some $\\bf{f} \\in \\mathbb{R}^n$. If we consider the Jacobi method, the iteration matrix $B_J = D^{-1}(E+F)$, where\n","\n","* D is the matric with only the diagnoal entries of A\n","* E is the lower triangular matrix of -A\n","* F is the upper triangular matrix of -A\n","\n","So we have \n","$$D = \\left( \\begin{matrix}\n","3 & 0 & 0 \\\\\n","0 & 3 & 0 \\\\\n","0 & 0 & 3 \\\\\n","\\end{matrix} \\right), \n","\n","E = \\left( \\begin{matrix}\n","0 & 0 & 0 \\\\\n","1 & 0& 0 \\\\\n","c & 0 & 0 \\\\\n","\\end{matrix} \\right), \n","\n","F = \\left( \\begin{matrix}\n","0 & 1 & c \\\\\n","0 & 0 & 0 \\\\\n","0 & 0 & 0 \\\\\n","\\end{matrix} \\right)\n","$$\n","\n","Since $D = 3I$, where $I$ is the identity matrix, we can easily find the inverse\n","\n","$$\n","D^{-1} = \\frac{1}{3}\\left( \\begin{matrix}\n","1 & 0 & 0 \\\\\n","0 & 1 & 0 \\\\\n","0 & 0 & 1 \\\\\n","\\end{matrix} \\right)\n","= \\left( \\begin{matrix}\n","\\frac{1}{3} & 0 & 0 \\\\\n","0 & \\frac{1}{3} & 0 \\\\\n","0 & 0 & \\frac{1}{3} \\\\\n","\\end{matrix} \\right)\n","$$\n","\n","Then,\n","\n","$$ B_J = D^{-1}(E+F) = \n","\\left( \\begin{matrix}\n","\\frac{1}{3} & 0 & 0 \\\\\n","0 & \\frac{1}{3} & 0 \\\\\n","0 & 0 & \\frac{1}{3} \\\\\n","\\end{matrix} \\right)\n","\\left( \\begin{matrix}\n","0 & 1 & c \\\\\n","1 & 0 & 0 \\\\\n","c & 0 & 0 \\\\\n","\\end{matrix} \\right)\n","=\n","\\left( \\begin{matrix}\n","0 & \\frac{1}{3} & \\frac{c}{3} \\\\\n","\\frac{1}{3} & 0 & 0 \\\\\n","\\frac{c}{3} & 0 & 0\\\\\n","\\end{matrix} \\right)\n","$$\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### b)\n","\n","We know that the Jacobi methods converges if  $\\rho(B_J) < 1$, meaning that the maximum of the absolute value of the eigenvalues of $B_J$ are less than one. To find for which $c$ the method converges, we have to compute the eigenvalues of $B_J$. \n","\n","$$\n","det(B_J - \\lambda I) = det \\left( \\begin{matrix}\n","-\\lambda & \\frac{1}{3} & \\frac{c}{3} \\\\\n","\\frac{1}{3} & -\\lambda & 0 \\\\\n","\\frac{c}{3} & 0 & -\\lambda\\\\\n","\\end{matrix} \\right)\n","= \\lambda (-\\lambda^2 + \\frac{\\lambda}{9}(c^2 +1))\n","$$\n","\n","So the eigenvalues are $\\lambda_1 = 0, \\lambda_2 = \\frac{\\sqrt{(c^2 +1)}}{3}$ and $\\lambda_3 = - \\frac{\\sqrt{(c^2 +1)}}{3}$.\n","\n","Thus, the Jacobi method converges if \n","\n","\\begin{align*}\n","&\\lvert\\frac{\\sqrt{c^2+1}}{3}\\rvert < 1 \\\\\n","&\\implies \\frac{\\sqrt{c^2+1}}{3} < 1 \\\\\n","&\\implies c^2 < 3-1 \\\\\n","&\\implies c < \\sqrt{2}\n","\\end{align*}\n","\n","The Jacobi method converges for all $c < \\sqrt{2} \\in \\mathbb{R}$."]},{"cell_type":"markdown","metadata":{},"source":["#### c)"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
