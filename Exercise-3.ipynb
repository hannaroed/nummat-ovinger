{"cells":[{"cell_type":"markdown","metadata":{},"source":["## TMA4215 - Exercise 3 ## \n","*Group: Hanna Heshmati Rød, Karine Austbø Grande and Thea Boge*"]},{"cell_type":"markdown","metadata":{},"source":["#### Problem 1"]},{"cell_type":"markdown","metadata":{},"source":["#### a)\n","\n","1. $ f(\\mathbf{x}) = \\|\\mathbf{x}\\|_2 $\n","\n","This is the Euclidean norm (also known as the $L_2$ norm) of the vector $\\mathbf{x}$, defined as:\n","$$\n","f(\\mathbf{x}) = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} = \\left( \\mathbf{x}^T \\mathbf{x} \\right)^{1/2}\n","$$\n","\n","So, we start with:\n","  $$\n","  f(\\mathbf{x}) = \\sqrt{\\mathbf{x}^T \\mathbf{x}}\n","  $$\n","\n","Now we differentiate with respect to $\\mathbf{x}$ using the chain rule. The derivative of $ \\sqrt{u} $ is $ \\frac{1}{2\\sqrt{u}} $:\n","  $$\n","  \\nabla f(\\mathbf{x}) = \\frac{1}{2 \\|\\mathbf{x}\\|_2} \\cdot 2\\mathbf{x} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n","  $$\n","\n","Thus, the gradient is:\n","$$\n","\\nabla f(\\mathbf{x}) = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n","$$\n","\n","\n","2. $ F(\\mathbf{x}) = A \\mathbf{x} $\n","\n","Here, $F(\\mathbf{x})$ is a matrix-vector multiplication, where $ A \\in \\mathbb{R}^{n \\times n} $.\n","\n","We need to compute the Jacobian matrix $ J_F(\\mathbf{x}) $. The Jacobian matrix for a vector-valued function is a matrix of partial derivatives.\n","  \n","Each component of $ F(\\mathbf{x}) = A \\mathbf{x} $ is a linear combination of the components of $\\mathbf{x}$, and the partial derivative of each output component with respect to each input component is the corresponding element of $A$.\n","  \n","In matrix form, the Jacobian is:\n","  $$\n","  J_F(\\mathbf{x}) = A\n","  $$\n","\n","Thus, the Jacobian matrix is simply the matrix $A$, because $A\\mathbf{x}$ is a linear transformation.\n","\n","\n","3. $ f(\\mathbf{x}) = \\|A \\mathbf{x}\\|_2^2 $\n","\n","This is the squared Euclidean norm of the vector $ A\\mathbf{x} $, defined as:\n","$$\n","f(\\mathbf{x}) = (A \\mathbf{x})^T (A \\mathbf{x}) = \\mathbf{x}^T A^T A \\mathbf{x}\n","$$\n","  \n","The gradient of a quadratic form $ \\mathbf{x}^T B \\mathbf{x} $ is given by:\n","  $$\n","  \\nabla_{\\mathbf{x}} \\left( \\mathbf{x}^T B \\mathbf{x} \\right) = 2 B \\mathbf{x}\n","  $$\n","  \n","Here, $ B = A^T A $, so the gradient is:\n","  $$\n","  \\nabla f(\\mathbf{x}) = 2 A^T A \\mathbf{x}\n","  $$\n","\n","4. $ f(\\mathbf{x}) = (\\mathbf{A}\\mathbf{x}, \\mathbf{x}) = \\mathbf{x}^T A^T \\mathbf{x} $\n","\n","This is a quadratic form involving the matrix $A^T$ and the vector $\\mathbf{x}$.\n","\n","We express the function as:\n","  $$\n","  f(\\mathbf{x}) = \\mathbf{x}^T A^T \\mathbf{x}\n","  $$\n","  \n","The gradient of a quadratic form $ \\mathbf{x}^T B \\mathbf{x} $ is given by:\n","  $$\n","  \\nabla_{\\mathbf{x}} \\left( \\mathbf{x}^T B \\mathbf{x} \\right) = (B + B^T) \\mathbf{x}\n","  $$\n","\n","We have $ B = A^T $, so the gradient is:\n","\n","  $$\n","  \\nabla f(\\mathbf{x}) = (A^T + A) \\mathbf{x}\n","  $$"]},{"cell_type":"markdown","metadata":{},"source":["#### b)\n","\n","If $ A $ is symmetric ($ A = A^T $), the gradient simplifies to:\n","$$\n","\\nabla f(\\mathbf{x}) = 2 A \\mathbf{x}\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["#### c)\n","\n","Assume $ A $ is symmetric, meaning $ A = A^T $.\n","\n","3. $ f(\\mathbf{x}) = \\|A \\mathbf{x}\\|_2^2 = \\mathbf{x}^T A^T A \\mathbf{x} $\n","\n","The gradient of the function, calculated earlier, is:\n","\n","$$\n","\\nabla f(\\mathbf{x}) = 2 A^T A \\mathbf{x} = 2 A^2 \\mathbf{x}\n","$$\n","\n","To compute the Hessian, we differentiate the gradient again with respect to $\\mathbf{x}$:\n","\n","$$\n","\\nabla^2 f(\\mathbf{x}) = 2 A^2\n","$$\n","\n","4. $ f(\\mathbf{x}) = (\\mathbf{A} \\mathbf{x}, \\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x} $\n","\n","The gradient of the given function, calculated previously, is:\n","\n","$$\n","\\nabla f(\\mathbf{x}) = 2 A \\mathbf{x}\n","$$\n","\n","Now we compute the Hessian by taking the gradient once again:\n","\n","$$\n","\\nabla^2 f(\\mathbf{x}) = 2 A\n","$$\n","\n","which is the Hessian for the fourth function."]},{"cell_type":"markdown","metadata":{},"source":["#### d)\n","\n","We are going to compute the gradient and Hessian of the following function:\n","\n","$$\n","f(\\mathbf{x}) = \\frac{1}{2} \\|A\\mathbf{x} - \\mathbf{y}\\|_2^2 + \\mu \\|\\mathbf{x}\\|_2^2\n","$$\n","\n","where $ A \\in \\mathbb{R}^{m \\times n} $, $ \\mathbf{x} \\in \\mathbb{R}^n $, $ \\mathbf{y} \\in \\mathbb{R}^m $, and $ \\mu > 0 $.\n","\n","First we expand the terms:\n","1. The first term: \n","   $$\n","   \\frac{1}{2} \\|A\\mathbf{x} - \\mathbf{y}\\|_2^2 = \\frac{1}{2} (A\\mathbf{x} - \\mathbf{y})^T (A\\mathbf{x} - \\mathbf{y})\n","   $$\n","   Expanding this:\n","   $$\n","   f_1(\\mathbf{x}) = \\frac{1}{2} \\left( \\mathbf{x}^T A^T A \\mathbf{x} - 2 \\mathbf{y}^T A \\mathbf{x} + \\mathbf{y}^T \\mathbf{y} \\right)\n","   $$\n","   The term $ \\mathbf{y}^T \\mathbf{y} $ is constant with respect to $\\mathbf{x}$, so we can ignore it in gradient and Hessian computations.\n","\n","2. The second term: \n","   $$\n","   \\mu \\|\\mathbf{x}\\|_2^2 = \\mu \\mathbf{x}^T \\mathbf{x}\n","   $$\n","\n","Now we compute the gradients:\n","\n","1. The first term:\n","\n","$$\n","\\nabla f_1(\\mathbf{x}) = A^T A \\mathbf{x} - A^T \\mathbf{y}\n","$$\n","\n","2. The second term:\n","\n","$$\n","\\nabla f_2(\\mathbf{x}) = 2 \\mu \\mathbf{x}\n","$$\n","\n","So the total gradient is:\n","\n","$$\n","\\nabla f(\\mathbf{x}) = A^T A \\mathbf{x} - A^T \\mathbf{y} + 2 \\mu \\mathbf{x}\n","$$\n","\n","Now we compute the Hessians:\n","\n","1. The first term:\n","\n","$$\n","\\nabla^2 f_1(\\mathbf{x}) = A^T A\n","$$\n","\n","2. The second term\n","\n","$$\n","\\nabla^2 f_2(\\mathbf{x}) = 2 \\mu I\n","$$\n","\n","Hence, the total Hessian is:\n","\n","$$\n","\\nabla^2 f(\\mathbf{x}) = A^T A + 2 \\mu I\n","$$"]},{"cell_type":"markdown","metadata":{},"source":["#### Problem 2\n","\n","We consider the linear system of equations $A\\bf{x} = \\bf{b}$, where\n","$$A = \\left( \\begin{matrix}\n","3 & -1 & -c \\\\\n","-1 & 3 & 0 \\\\\n","-c & 0 & 3 \\\\\n","\\end{matrix} \\right)$$\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### a)\n","We want to reformulate the system of equations as a fix point equation \n","$$ A\\bf{x} = \\bf{b} \\iff  \\bf{x} = B\\bf{x} + \\bf{f}$$\n","with the iteration matrix B and some $\\bf{f} \\in \\mathbb{R}^n$. If we consider the Jacobi method, the iteration matrix $B_J = D^{-1}(E+F)$, where\n","\n","* D is the matric with only the diagnoal entries of A\n","* E is the lower triangular matrix of -A\n","* F is the upper triangular matrix of -A\n","\n","So we have \n","$$D = \\left( \\begin{matrix}\n","3 & 0 & 0 \\\\\n","0 & 3 & 0 \\\\\n","0 & 0 & 3 \\\\\n","\\end{matrix} \\right), \n","\n","E = \\left( \\begin{matrix}\n","0 & 0 & 0 \\\\\n","1 & 0& 0 \\\\\n","c & 0 & 0 \\\\\n","\\end{matrix} \\right), \n","\n","F = \\left( \\begin{matrix}\n","0 & 1 & c \\\\\n","0 & 0 & 0 \\\\\n","0 & 0 & 0 \\\\\n","\\end{matrix} \\right)\n","$$\n","\n","Since $D = 3I$, where $I$ is the identity matrix, we can easily find the inverse\n","\n","$$\n","D^{-1} = \\frac{1}{3}\\left( \\begin{matrix}\n","1 & 0 & 0 \\\\\n","0 & 1 & 0 \\\\\n","0 & 0 & 1 \\\\\n","\\end{matrix} \\right)\n","= \\left( \\begin{matrix}\n","\\frac{1}{3} & 0 & 0 \\\\\n","0 & \\frac{1}{3} & 0 \\\\\n","0 & 0 & \\frac{1}{3} \\\\\n","\\end{matrix} \\right)\n","$$\n","\n","Then,\n","\n","$$ B_J = D^{-1}(E+F) = \n","\\left( \\begin{matrix}\n","\\frac{1}{3} & 0 & 0 \\\\\n","0 & \\frac{1}{3} & 0 \\\\\n","0 & 0 & \\frac{1}{3} \\\\\n","\\end{matrix} \\right)\n","\\left( \\begin{matrix}\n","0 & 1 & c \\\\\n","1 & 0 & 0 \\\\\n","c & 0 & 0 \\\\\n","\\end{matrix} \\right)\n","=\n","\\left( \\begin{matrix}\n","0 & \\frac{1}{3} & \\frac{c}{3} \\\\\n","\\frac{1}{3} & 0 & 0 \\\\\n","\\frac{c}{3} & 0 & 0\\\\\n","\\end{matrix} \\right)\n","$$\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### b)\n","\n","We know that the Jacobi methods converges if  $\\rho(B_J) < 1$, meaning that the maximum of the absolute value of the eigenvalues of $B_J$ are less than one. To find for which $c$ the method converges, we have to compute the eigenvalues of $B_J$. \n","\n","$$\n","det(B_J - \\lambda I) = det \\left( \\begin{matrix}\n","-\\lambda & \\frac{1}{3} & \\frac{c}{3} \\\\\n","\\frac{1}{3} & -\\lambda & 0 \\\\\n","\\frac{c}{3} & 0 & -\\lambda\\\\\n","\\end{matrix} \\right)\n","= \\lambda (-\\lambda^2 + \\frac{\\lambda}{9}(c^2 +1))\n","$$\n","\n","So the eigenvalues are $\\lambda_1 = 0, \\lambda_2 = \\frac{\\sqrt{(c^2 +1)}}{3}$ and $\\lambda_3 = - \\frac{\\sqrt{(c^2 +1)}}{3}$.\n","\n","Thus, the Jacobi method converges if \n","\n","\\begin{align*}\n","&\\lvert\\frac{\\sqrt{c^2+1}}{3}\\rvert < 1 \\\\\n","&\\implies \\frac{\\sqrt{c^2+1}}{3} < 1 \\\\\n","&\\implies c^2 < 3-1 \\\\\n","&\\implies c < \\sqrt{2}\n","\\end{align*}\n","\n","The Jacobi method converges for all $c < \\sqrt{2} \\in \\mathbb{R}$."]},{"cell_type":"markdown","metadata":{},"source":["#### c)\n","\n","Now we want to prove that the Gauss-Seidel method converges for any $c \\in \\left[-1,1\\right]$. We know that the Gauss-Siedel method converges if $A$ is strictly diagonal dominant, i.e. if $|a_{ii}| > \\sum_{i\\neq j} |a_{ij}|$.\n","\n","Considering the matrix \n","\n","$$A = \\left( \\begin{matrix}\n","3 & -1 & -c \\\\\n","-1 & 3 & 0 \\\\\n","-c & 0 & 3 \\\\\n","\\end{matrix} \\right)$$\n","\n","we see that all diagnoal entries are 3, so we need $\\sum_{i\\neq j} |a_{ij}| < 3$, which is true for all $c \\in \\left[-1,1\\right]$. \n","\n","$\\square$"]},{"cell_type":"markdown","metadata":{},"source":["### Problem 3\n","\n","consider the matrix $A\\in\\mathbb{R}^{nxn}$\n","Let $\\bold{b} = (1,...,1)^{T} \\in \\mathbb{R}^{n}$\n","\n","We are going to define each algoritm, and create a function with identical interface, that is, a funciton that takes in the same arguments and returns the same output.\n","\n","Each algorithm stop if either the realtive error satisfies $ \\dfrac{||\\bold{r}^{(k)}||}{||\\bold{r}^{(0)}||} < \\epsilon$, where $\\epsilon$ is our tolerance, or a maximum number of iterations maxiter is reached. "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import numpy as np\n","import time\n","\n","n = 2000\n","tol = 1e-7\n","maxiter = 5000\n","\n","b = np.ones(n)\n","\n","A = np.zeros((n,n))\n","for i in range(n):\n","    for j in range(n):\n","        A[i,j] = 3 **(-abs(i-j)) + 2**(-(i+j)) + 1e-6 * np.random.uniform()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Richardson iteration\n","\n","def richardson(A, b):\n","    omega = 1\n","    x = np.copy(b)\n","    r = b - A @ x\n","    r0 = np.copy(r)\n","    count = 0\n","\n","    while np.linalg.norm(r) / np.linalg.norm(b) > tol and count < maxiter:\n","        \n","        r = b - A @ x\n","        x += omega * r\n","        count += 1\n","\n","    return x, count, np.linalg.norm(r)/np.linalg.norm(r0)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Gauss-Seidel method\n","\n","def gausseidel(A,b):\n","    x = np.copy(b)\n","    r = b - A @ x\n","    r0 = np.copy(r)\n","    count = 0\n","\n","    while np.linalg.norm(r) / np.linalg.norm(b) > tol and count < maxiter:\n","        for i in range(n):\n","            sum1 = np.dot(A[i, :i], x[:i])\n","            sum2 = np.dot(A[i, i+1:], x[i+1:])\n","            x[i] = (b[i] - sum1 - sum2) / A[i,i]\n","        r = b - A @ x\n","        count += 1\n","\n","    return x,count, np.linalg.norm(r)/np.linalg.norm(r0)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Gradient Descent with \"perfect\" line search\n","\n","def steepestdescent(A,b):\n","    x = np.copy(b)\n","    r = b - A @ x\n","    r0 = np.copy(r)\n","    count = 0\n","\n","    while np.linalg.norm(r) / np.linalg.norm(b) > tol and count < maxiter:\n","        Ar = A @ r0\n","        omega = np.dot(r,r) / np.dot(r, Ar)\n","        x += omega * r\n","        r = r - omega * Ar\n","        count += 1\n","    return x,count, np.linalg.norm(r)/np.linalg.norm(r0)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Running Richardson method...\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/n2/1nxnxr553t79zck8_31l8sph0000gn/T/ipykernel_89322/2974712515.py:13: RuntimeWarning: invalid value encountered in add\n","  x += omega * r\n"]},{"name":"stdout","output_type":"stream","text":["--- Richardson Results ---\n","Iterations: 1182\n","Final Relative Residual: nan\n","Time Taken: 0.6165 seconds\n","------------------------------\n","Running Gauss-Seidel method...\n","--- Gauss-Seidel Results ---\n","Iterations: 15\n","Final Relative Residual: 6.97e-08\n","Time Taken: 0.0758 seconds\n","------------------------------\n","Running Gradient Descent method...\n","--- Gradient Descent Results ---\n","Iterations: 5000\n","Final Relative Residual: 1.00e+00\n","Time Taken: 2.5690 seconds\n","------------------------------\n","\n","Summary of Results:\n","Method              Iterations  Final Residual      Time (seconds) \n","===================================================================\n","Richardson          1182        nan                 0.6165         \n","Gauss-Seidel        15          6.97e-08            0.0758         \n","Gradient Descent    5000        1.00e+00            2.5690         \n"]}],"source":["methods = {\n","    \"Richardson\": richardson,\n","    \"Gauss-Seidel\": gausseidel,\n","    \"Gradient Descent\": steepestdescent\n","}\n","\n","results = []\n","\n","for name, method in methods.items():\n","    print(f'Running {name} method...')\n","    start_time = time.time()\n","    x, iterations, rel_res = method(A, b)\n","    duration = time.time() - start_time\n","    \n","    # Append the results\n","    results.append((name, iterations, rel_res, duration))\n","    \n","    # Print results in a clear format\n","    print(f'--- {name} Results ---')\n","    print(f'Iterations: {iterations}')\n","    print(f'Final Relative Residual: {rel_res:.2e}')\n","    print(f'Time Taken: {duration:.4f} seconds')\n","    print('-' * 30)\n","\n","# Optionally: Print all results in a summary table format\n","print(\"\\nSummary of Results:\")\n","print(f\"{'Method':<20}{'Iterations':<12}{'Final Residual':<20}{'Time (seconds)':<15}\")\n","print(\"=\"*67)\n","for name, iterations, rel_res, duration in results:\n","    print(f\"{name:<20}{iterations:<12}{rel_res:<20.2e}{duration:<15.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":2}
